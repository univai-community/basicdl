{"cells":[{"cell_type":"markdown","metadata":{},"source":["# The Multi-Layer Perceptoron\n","\n","![](images/perceptron.png)\n","\n","\n","We'll start by modifying the logistic regression structure somewhat, to use another non-linearity, rather than a sigmoid or softmax. We typically use a Relu function:\n","\n","![](https://miro.medium.com/max/1400/1*XxxiA0jJvPrHEJHD4z893g.png)\n","\n","At this moment this is not a classification unit, but a compute unit in a larger structure, called a peceptron. We will combine perceptron's together to create a neural network, called a *Multi-Layer Perceptron*.\n","\n","For example, we can combine 2 perceptons, and then repeat the process:\n","\n","![](images/mlp.png)\n","\n","We can combine many more:\n","\n","![](images/Figure-18-032.png)\n","\n","Indeed such combination can be made:\n","\n","- both deep and wide\n","- this buys us complex nonlinearity\n","- both for regression and classification\n","- key technical advance: BackPropagation with autodiff\n","- key technical advance: gpu\n","\n","Why does this work? There is a Universal Approximation theorem that says that a network with one hidden layer can approximate any continuous function with finite support, with appropriate choice of nonlinearity\n","\n","- under appropriate conditions, all of sigmoid, tanh, RELU can work\n","- but may need lots of units\n","- and will learn the function it thinks the data has, not what you think\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## MLP on the MNIST dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":239,"status":"ok","timestamp":1674093930260,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"GvoP18I-mesM"},"outputs":[],"source":["from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from keras.utils import to_categorical\n","from keras.utils import np_utils"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1674093931501,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"uRcYZzOqxTvi"},"outputs":[],"source":["class Config:\n","  pass\n","config = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1674093933394,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"8q5yDjXHxEGm"},"outputs":[],"source":["config.optimizer = \"adam\"\n","config.epochs = 10\n","config.hidden_nodes = 100\n","\n","# load data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","img_width = X_train.shape[1]\n","img_height = X_train.shape[2]\n","\n","X_train = X_train.astype('float32')\n","X_train /= 255.\n","X_test = X_test.astype('float32')\n","X_test /= 255.\n","\n","# one hot encode outputs\n","y_train = np_utils.to_categorical(y_train)\n","y_test = np_utils.to_categorical(y_test)\n","labels = range(10)\n","\n","num_classes = y_train.shape[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3506,"status":"ok","timestamp":1674093953847,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"_WDrb_qtxcvt","outputId":"2caefc96-b67c-467f-922a-982196ece8a6"},"outputs":[],"source":["# create model\n","model = Sequential()\n","model.add(Flatten(input_shape=(img_width, img_height)))\n","model.add(Dense(config.hidden_nodes, activation='relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer=config.optimizer,\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## A segue on SGD\n","\n","SGD has a very simple idea: dont construct the loss surface from the full data. Take a **batch** of data instead, of sixe $n$:\n","\n","$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} {\\cal L}(\\theta_t; x^{(i:i+n)}; y^{(i:i+n)})$$\n","\n","The big idea here is that at each batch, you have a **new loss surface**.\n","\n","```python\n","for i in range(n_epochs):\n","  np.random.shuffle(data) # so that data is seen in different orders leading to more loss surfaces\n","  for batch in get_batches(data, batch_size=50):\n","    params_grad = evaluate_gradient(loss_function, batch, params)\n","    params = params - learning_rate * params_grad\n","```\n","\n","Indeed, SGD **changes the loss surface**\n","\n","One can see this even for the convex linear regression loss, where the different surfaces look like this.\n","\n","![fit, inline](images/animsgd.gif)\n","\n","SGD is not useful for convex surfaces, as you will always get to the bottom. But the losses for the non-linear neural nets ARE NOT CONVEX. Indeed they are like the broken up everest, lhotse, nuptse valleys we saw before.\n","\n","Here SGD achieves something amazing for us:\n","\n","\n","![left, fit](images/flbl.png)\n","\n","Remember you want to find the best minimum of the loss.\n","\n","- If losses are not convex, simple gradient descent can get trapped (unless the learning rate is high)\n","- but if the learning rate is always high, you might lose good minima\n","- by using only some data you change the surface, and this may release you from the trap\n","- thus you can avoid shallow local minima and make your way towards something deeper\n","\n","SGD does not finding the global minimum, but is unreasonably effective in getting us close."]},{"cell_type":"markdown","metadata":{},"source":["## Back to the fit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82919,"status":"ok","timestamp":1674094061724,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"ZzrmTbKDxnKY","outputId":"ab1f0997-8fe1-4f27-852b-2f9964713cc6"},"outputs":[],"source":["history = model.fit(X_train, y_train, validation_data=(X_test, y_test),\n","          epochs=config.epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":797,"status":"ok","timestamp":1674094206004,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"7JRf1Ob8xuKu","outputId":"663faf87-38c7-4594-ffac-c26caee9dae8"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":696,"status":"ok","timestamp":1674094233302,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"_XMO9Eg3ylXp","outputId":"5b430e77-ae85-4ec0-f6fd-f505ab3b859e"},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["One notices that after some epochs of training, the validation set accuracy falls below the training set accuracy, and vice-versa for the loss.\n","\n","This is overfitting. We are not able to do as well on the unseen data (validation set) as we did on the data we trained with. This is bad in any machine learning model as we overpromise but under-deliver, and in the extreme case, our predictions are not robust.\n","\n","This overfitting can be seen even more clearly in this set of images which uses multilayer perceptons in the 2D half-moon dataset you can generate in `sklearn`\n","\n","First the dataset followed by logistic regression:\n","\n","![inline](images/halfmoonsset.png)![inline](images/mlplogistic.png)\n","\n","Now consider a 1 hidden layer MLP with 2 vs 10 neurons:\n","\n","![inline](images/mlp2102.png.png)![inline](images/mlp2110.png)\n","\n","\n","Finally, a 2-layer MLP, with 20 neurons per layer vs  5 layers, with 1000 neurons per layer\n","\n","![inline](images/mlp2220.png)![inline](images/mlp251000.png)\n","\n","You can clearly see that as the complexity of the model becomes too much, it no longer does as well."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN/7ePq4j3QN5RjxV930xUs","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.11.1 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.1"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":0}
