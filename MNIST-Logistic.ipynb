{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression\n","\n","$$\\renewcommand{\\v}[1]{\\mathbf #1}$$\n","\n","The basic idea is to model the probability of $y=1$ at a given $x$ as a sigmoid:\n","\n","$$p(y=1 \\mid x) = \\frac{1}{1+e^{-\\v{w} \\cdot \\v{x}}}$$\n","\n","![](images/1dclasslogisticprobs.png)\n","\n","One can write the process schematically as so:\n","\n","![inline](images/layershorstd.png)\n","\n","One can switch the sigmoid formulation to the so-called **Softmax** formulation. This helps generalize to more than one class.\n","\n","## Softmax formulation\n","\n","Identify $$p_i$$ and $$1-p_i$$ as two separate probabilities constrained to add to 1. \n","\n","That is $$p_{1i} = p_i ; p_{2i} = 1 - p_i. $$\n","\n","$$p_{1i} = \\frac{e^{\\v{w_1} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}$$\n","\n","$$p_{2i} = \\frac{e^{\\v{w_2} \\cdot \\v{x}}}{e^{\\v{w_1} \\cdot \\v{x}} + e^{\\v{w_2} \\cdot \\v{x}}}$$\n","\n","Notice that one can translate coefficients by fixed amount $\\v{\\psi}$ without any change (if you change $\\v{w}$ to $\\v{w} + \\v{\\psi}$ there is no change in the probabilities). \n","\n","Thus, by setting $\\v{\\psi}$ to $-\\v{w_1}$ we regain the sigmoid with $\\v{w} = \\v{w_1} -\\v{w_2}$.\n","\n","\n","Since \n","\n","$${\\cal L} = \\prod_i p_{1i}^{\\mathbb1_1(y_i)} p_{2i}^{\\mathbb1_2(y_i)}$$ \n","\n","where ${\\mathbb1_1(y_i)}$ is a function which is 1 if $y_i$ is classified as the first class. In other words, if $y_i$ is written as a one-hot encoded vector, the plce corresponding to the first class has a 1 while the others have 0.\n","\n","we can derive the cross-entropy loss, or negative log likelihood:\n","\n","$$NLL = -\\sum_i \\left( \\mathbb1_1(y_i) log(p_{1i}) + \\mathbb1_2(y_i) log(p_{2i}) \\right)$$\n","\n","![inline](images/layershorsm.png)\n"]},{"cell_type":"markdown","metadata":{},"source":["## The MNIST dataset\n","\n","This is a dataset of handwritten digits from the US Postal service. Automatic sorting machines want to read these and route mail. The are 10 classes corresponding to the 10 fundamental digits."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":148,"status":"ok","timestamp":1674093675115,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"N1YCaKhYwaNT"},"outputs":[],"source":["from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":514,"status":"ok","timestamp":1674093718161,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"cPZn5iN-vzup","outputId":"c6740fc3-cdad-4e19-acb5-c46726345817"},"outputs":[],"source":["# load data\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","img_width = X_train.shape[1]\n","img_height = X_train.shape[2]\n","\n","# one hot encode outputs\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","labels = range(10)\n","\n","num_classes = y_train.shape[1]\n","\n","\n","X_train[0].shape, y_train[0]"]},{"cell_type":"markdown","metadata":{},"source":["We will create 784 features, one from each pixel, and then do a linear function on these 784 features. Thus there are 784 slopes and a bias. We will do this once for each of the 10 classes as in the diagram above. Hence we have 785 times 10, or 7850 features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":533,"status":"ok","timestamp":1674093747526,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"jQhu05aGwKwx","outputId":"cb145c22-cada-41b6-e7cd-dd6ea80e06e8"},"outputs":[],"source":["# create model\n","model = Sequential()\n","model.add(Flatten(input_shape=(img_width, img_height)))\n","model.add(Dense(num_classes, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam',\n","              metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["You will notice that we are fitting the model vis an algorithm called `Adam`, a variant of an algorithm called SGD. Lets take a bit of a detour into our fitting method. "]},{"cell_type":"markdown","metadata":{},"source":["## Descending fast from Mount Everest"]},{"cell_type":"markdown","metadata":{},"source":["\n","![right, fit](images/lfrome.jpg)\n","\n","Wherever you are on the mountain, plunge down the steepest direction you possibly can.\n","\n","You will make it down to a valley, but perhaps not the one you want to be in.\n","\n","Your step size is called the **learning rate**.\n","\n","Too small a step size and you will freeze.\n","\n","But if you are a giant with a large step size you might just step into another valley.\n","\n","Where do you land up going?\n","\n","![right, fit](images/e2d.jpg)\n","\n","To find the steepest descent, find the place with the highest \"slope\", or highest derivative.\n","\n","To go downwards, go in the opposite direction to this highest derivative.\n","\n","A step to the right from the top of Everest will make you go down into the Western cwm. If you are an ant, you have too small a step: you run out of oxygen. But if you are a giant roaming the earth you could step past Nuptse!\n","\n","\n","## Back to Logistic Regression What is this loss geography?\n","\n","For the Linear regression Loss function:\n","\n","$${\\cal L} =  \\frac{1}{N}\\sum_{i = 1}^{N} (y_i - (m\\,x_i + b))^2 $$\n","\n","or for the cross-entropy, what does our loss function look like? (let us assume only one $\\v{w}$ with 2 parameters $m$ and $b$ for now)\n","\n","The loss here is a function of these **parameters** of the model. This is the latitude and longitude equivalents in our loss landscape,\n","\n","The contours are those of constant loss, just as in a map it is constant altitude. The gradient is perpendicular to the contours.\n","\n","For both linear regression and logistic regression these contours look like this:\n","\n","![right, fit](images/linregcontour.png)\n","\n","Both of these are **convex** losses. A bowl. You always find the bottom.\n","\n","![inline](images/3danim010.png)\n","\n","\n","\n","## More details: Gradient Descent in 1-D\n","\n","**go opposite the direction of the derivative.**\n","\n","Consider the objective function: $ J(x) = x^2-6x+5 $\n","\n","Its derivative or **gradient** is $2x - 6$. This is positive for $x>3$. Negative below.\n","\n","Now do:\n","\n","```python\n","gradient = fprime(old_x)\n","move = gradient * step\n","current_x = old_x - move\n","```\n","\n","![right, fit](images/optimcalc_4_0.png)\n","\n","\n","\n","- For $x > 3$, $step \\times (2x-6)$ is positive, and so you go to smaller (leftward) $x$ by $step \\times (2x-6)$.\n","- The moment $x < 3$, you are on the other side of the parabola, the derivative is negative, and so you go rightward by $step \\times (2x-6)$.\n","- How much do you go? Depends on step size.\n","\n","\n","```python\n","move = (2x - 6) * step\n","current_x = old_x - move\n","```\n","\n","What is the impact of step size?\n","\n","#### too big step size\n","\n","![inline](images/1dgd-bigstep_AdobeExpress.gif)\n","\n","\n","\n","#### too small step size\n","\n","![inline](images/1dgd-smallstep_AdobeExpress.gif)\n","\n","\n","\n","#### good step size\n","\n","![inline](images/1dgd_AdobeExpress.gif)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","### Gradient Descent more formally\n","\n","$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} {\\cal L}(\\theta_t)$$\n","\n","where $\\eta$ is the learning rate, $\\theta$ represents the parameters. The symbol $\\nabla$ represents the gradient.\n","\n","ENTIRE DATASET NEEDED\n","\n","```python\n","for i in range(n_epochs):\n","  params_grad = evaluate_gradient(loss_function, data, params)\n","  params = params - learning_rate * params_grad`\n","```\n","\n","On so this is neither SGD, nor Adam, which we will come to later. But Gradient Descent is all we need for linear and logistic regression."]},{"cell_type":"markdown","metadata":{},"source":["## Back to the fit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42809,"status":"ok","timestamp":1674093819448,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"BLpr4Ouwwiqy","outputId":"aa9d5c52-99bb-4438-8c58-ae5c65f59a46"},"outputs":[],"source":["# Fit the model\n","history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":791,"status":"ok","timestamp":1674094085970,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"jJMwiBEsw22u","outputId":"cd9436d2-3994-4636-b68f-8eb80f5007b7"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":647,"status":"ok","timestamp":1674094112036,"user":{"displayName":"Rahul Dave","userId":"03742901065991133251"},"user_tz":300},"id":"6LSFZXCFx0JP","outputId":"06b94cbd-f77d-4f9e-fbdc-8ac55f3bdb00"},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPCkLcF3Ci82ADpYNWoNMhV","provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('ml1-arm64')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"b2dbb87eb36a1fdea54255e015681f8e0915a5f1c11d9f7f5c9cdf0937e9541e"}}},"nbformat":4,"nbformat_minor":0}
